{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2b2be3",
   "metadata": {},
   "source": [
    "\n",
    "# Google Cluster Data — Starter EDA\n",
    "\n",
    "This notebook loads a small sample of the **Google Cluster Data (2019)** `task_usage` table and performs first-pass EDA suitable for cloud autoscaling research.\n",
    "\n",
    "**Expected file (created by your setup script):**\n",
    "```\n",
    "google_cluster_data/data_sample/part-00000-of-00500.csv\n",
    "```\n",
    "If the path differs, update the `DATA_PATH` variable below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dcc27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Inline figures\n",
    "%matplotlib inline\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "DATA_PATH = \"google_cluster_data/data_sample/part-00000-of-00500.csv\"\n",
    "assert os.path.exists(DATA_PATH), f\"Data file not found at {DATA_PATH}. Please run the setup script or correct the path.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a01dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a subset first for speed; adjust nrows=None to load full file\n",
    "nrows = 500000  # change to None for entire file\n",
    "df = pd.read_csv(DATA_PATH, nrows=nrows)\n",
    "df.shape, df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60567e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nDataFrame shape:\", df.shape)\n",
    "print(\"\\nNull counts:\\n\", df.isna().sum().sort_values(ascending=False).head(20))\n",
    "df.describe(include='all').T.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb79b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "candidate_numeric = ['cpu_rate', 'mem_usage', 'disk_io_time', 'disk_space', 'assigned_memory', 'page_cache']\n",
    "numeric_cols = [c for c in candidate_numeric if c in df.columns]\n",
    "print(\"Numeric columns found:\", numeric_cols)\n",
    "df[numeric_cols].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c62223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(numeric_cols) >= 2:\n",
    "    corr = df[numeric_cols].corr()\n",
    "    corr\n",
    "else:\n",
    "    print(\"Not enough numeric columns found for correlation matrix.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db0083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure()\n",
    "    df[col].dropna().hist(bins=50)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb895c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Many Google traces store time in nanoseconds since epoch. Adjust if schema differs.\n",
    "for tcol in ['start_time', 'end_time']:\n",
    "    if tcol in df.columns:\n",
    "        # Heuristic: if values are very large, assume nanoseconds\n",
    "        if df[tcol].dropna().astype(float).mean() > 1e12:\n",
    "            df[tcol] = pd.to_datetime(df[tcol], unit='ns', origin='unix', errors='coerce')\n",
    "        else:\n",
    "            # if already seconds\n",
    "            df[tcol] = pd.to_datetime(df[tcol], unit='s', origin='unix', errors='coerce')\n",
    "\n",
    "time_cols = [c for c in ['start_time', 'end_time'] if c in df.columns]\n",
    "time_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'start_time' in df.columns and 'cpu_rate' in df.columns:\n",
    "    ts = df[['start_time', 'cpu_rate']].dropna()\n",
    "    ts = ts.set_index('start_time').sort_index()\n",
    "    # Resample to 1-hour mean if frequency supports it\n",
    "    hourly = ts['cpu_rate'].resample('1H').mean()\n",
    "    display(hourly.head())\n",
    "    plt.figure()\n",
    "    hourly.plot()\n",
    "    plt.title(\"Mean CPU rate by hour\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Mean CPU rate\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Missing 'start_time' and/or 'cpu_rate' for time-based aggregation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'start_time' in df.columns and 'cpu_rate' in df.columns:\n",
    "    # Downsample to 5-minute intervals for example\n",
    "    ts5 = df[['start_time','cpu_rate']].dropna().set_index('start_time').sort_index()['cpu_rate'].resample('5T').mean()\n",
    "    ts5 = ts5.interpolate(limit_direction='both')\n",
    "    lagged = pd.DataFrame({\n",
    "        'cpu_t': ts5,\n",
    "        'cpu_t_1': ts5.shift(1),\n",
    "        'cpu_t_2': ts5.shift(2),\n",
    "        'cpu_t_12': ts5.shift(12)  # one hour back if 5T frequency\n",
    "    }).dropna()\n",
    "    display(lagged.head())\n",
    "    print(\"\\nLag correlations:\")\n",
    "    display(lagged.corr())\n",
    "else:\n",
    "    print(\"Cannot compute lag features without 'start_time' and 'cpu_rate'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "group_keys = [c for c in ['machine_id', 'job_id', 'task_index'] if c in df.columns]\n",
    "if 'cpu_rate' in df.columns and len(group_keys) > 0:\n",
    "    key = group_keys[0]\n",
    "    g = df[[key, 'cpu_rate']].dropna().groupby(key)['cpu_rate'].mean().sort_values(ascending=False).head(10)\n",
    "    print(f\"Top 10 {key} by mean CPU_rate:\")\n",
    "    display(g)\n",
    "else:\n",
    "    print(\"No grouping keys ('machine_id', 'job_id', 'task_index') found for grouping demo, or 'cpu_rate' missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_parquet = \"google_cluster_data/data_sample/sample_eda.parquet\"\n",
    "small = df.sample(min(len(df), 200000), random_state=17)\n",
    "small.to_parquet(out_parquet, index=False)\n",
    "print(f\"Saved sample to {out_parquet}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc41fee",
   "metadata": {},
   "source": [
    "\n",
    "## Next Steps\n",
    "\n",
    "- Expand the sample: load additional `task_usage` parts and concatenate.\n",
    "- Join with `task_events` and `machine_events` (if available) to enrich context (scheduling, capacity changes).\n",
    "- Engineer autoscaling features (rolling means, percentiles, sustained high-CPU durations).\n",
    "- Prototype policies:\n",
    "  - Threshold-based: scale up when CPU > X% for Y minutes; scale down when < Z% for Y minutes.\n",
    "  - Predictive: forecast CPU 5–15 minutes ahead and make proactive decisions.\n",
    "  - RL: define state (recent utilization window), actions (scale up/down/hold), reward (SLO adherence minus cost).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

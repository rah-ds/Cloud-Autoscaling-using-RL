{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "754861b7",
   "metadata": {},
   "source": [
    "\n",
    "# Google Cluster Data â€” Colab EDA (Auto-Download)\n",
    "\n",
    "This notebook is designed for **Google Colab**. It will:\n",
    "- Create a workspace under `/content/google_cluster_data`\n",
    "- **Download** the 2019 Google Cluster Data schema and a small sample of `task_usage`\n",
    "- **Decompress** the sample\n",
    "- Run first-pass EDA geared toward **cloud autoscaling**\n",
    "\n",
    "> Tip: If you want files to persist, mount Google Drive in the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Optional) Mount Google Drive to persist files between sessions.\n",
    "# Uncomment the next two lines if you'd like to save under /content/drive/MyDrive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec241c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, gzip, shutil, urllib.request, subprocess, io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matplotlib inline in Colab\n",
    "%matplotlib inline\n",
    "\n",
    "# Choose a root directory; change to a Drive path if mounted, e.g. '/content/drive/MyDrive/google_cluster_data'\n",
    "ROOT_DIR = \"/content/google_cluster_data\"\n",
    "DOCS_DIR = os.path.join(ROOT_DIR, \"docs\")\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data_sample\")\n",
    "\n",
    "SCHEMA_URL = \"https://storage.googleapis.com/google-clusterdata-2019/schema.csv\"\n",
    "SAMPLE_URL = \"https://storage.googleapis.com/google-clusterdata-2019/task_usage/part-00000-of-00500.csv.gz\"\n",
    "\n",
    "SCHEMA_OUT = os.path.join(DOCS_DIR, \"schema.csv\")\n",
    "SAMPLE_GZ_OUT = os.path.join(DATA_DIR, \"part-00000-of-00500.csv.gz\")\n",
    "SAMPLE_CSV_OUT = os.path.join(DATA_DIR, \"part-00000-of-00500.csv\")\n",
    "\n",
    "os.makedirs(DOCS_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "def _have(cmd):\n",
    "    try:\n",
    "        subprocess.run([cmd, \"--version\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=False)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def download(url, out_path):\n",
    "    \"\"\"Download URL to out_path using wget/curl if available; otherwise urllib.\"\"\"\n",
    "    if _have(\"wget\"):\n",
    "        code = subprocess.call([\"wget\", \"-q\", \"-O\", out_path, url])\n",
    "        if code == 0 and os.path.exists(out_path):\n",
    "            return\n",
    "    if _have(\"curl\"):\n",
    "        code = subprocess.call([\"curl\", \"-L\", \"-sS\", \"-o\", out_path, url])\n",
    "        if code == 0 and os.path.exists(out_path):\n",
    "            return\n",
    "    # Fallback: urllib\n",
    "    with urllib.request.urlopen(url) as r, open(out_path, \"wb\") as f:\n",
    "        shutil.copyfileobj(r, f)\n",
    "\n",
    "print(\"==> Downloading schema...\")\n",
    "download(SCHEMA_URL, SCHEMA_OUT)\n",
    "print(\"    Wrote:\", SCHEMA_OUT, os.path.getsize(SCHEMA_OUT), \"bytes\")\n",
    "\n",
    "print(\"==> Downloading sample .gz...\")\n",
    "download(SAMPLE_URL, SAMPLE_GZ_OUT)\n",
    "print(\"    Wrote:\", SAMPLE_GZ_OUT, os.path.getsize(SAMPLE_GZ_OUT), \"bytes\")\n",
    "\n",
    "print(\"==> Decompressing sample .gz to CSV...\")\n",
    "with gzip.open(SAMPLE_GZ_OUT, \"rb\") as gz, open(SAMPLE_CSV_OUT, \"wb\") as out:\n",
    "    shutil.copyfileobj(gz, out)\n",
    "\n",
    "print(\"    Wrote:\", SAMPLE_CSV_OUT, os.path.getsize(SAMPLE_CSV_OUT), \"bytes\")\n",
    "\n",
    "print(\"\\nAll downloads complete. Paths ready:\")\n",
    "print(\"  Schema:\", SCHEMA_OUT)\n",
    "print(\"  Sample CSV:\", SAMPLE_CSV_OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a0b8a",
   "metadata": {},
   "source": [
    "\n",
    "## Load Data\n",
    "Reads a subset first for speed. Set `nrows=None` to load the entire sample file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc273759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = \"/content/google_cluster_data/data_sample/part-00000-of-00500.csv\"\n",
    "assert os.path.exists(DATA_PATH), f\"Data file not found at {DATA_PATH}\"\n",
    "nrows = 500_000  # set to None to read the full file\n",
    "df = pd.read_csv(DATA_PATH, nrows=nrows)\n",
    "df.shape, df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f2871",
   "metadata": {},
   "source": [
    "\n",
    "## Basic Info & Nulls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b89467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nShape:\", df.shape)\n",
    "print(\"\\nNull counts (top 20):\\n\", df.isna().sum().sort_values(ascending=False).head(20))\n",
    "df.describe(include='all').T.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e71ead",
   "metadata": {},
   "source": [
    "\n",
    "## Numeric Columns (Common Autoscaling Signals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "candidate_numeric = ['cpu_rate', 'mem_usage', 'disk_io_time', 'disk_space', 'assigned_memory', 'page_cache']\n",
    "numeric_cols = [c for c in candidate_numeric if c in df.columns]\n",
    "print(\"Numeric columns found:\", numeric_cols)\n",
    "\n",
    "if numeric_cols:\n",
    "    display(df[numeric_cols].describe())\n",
    "else:\n",
    "    print(\"No expected numeric columns found; check schema and column names.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec494b0",
   "metadata": {},
   "source": [
    "\n",
    "## Correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de08c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(numeric_cols) >= 2:\n",
    "    corr = df[numeric_cols].corr()\n",
    "    corr\n",
    "else:\n",
    "    print(\"Not enough numeric columns to compute correlations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f0db2",
   "metadata": {},
   "source": [
    "\n",
    "## Distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a7474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure()\n",
    "    df[col].dropna().hist(bins=50)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e572dd",
   "metadata": {},
   "source": [
    "\n",
    "## Time Conversion\n",
    "Attempts to convert `start_time` / `end_time` to datetimes. The trace often uses **nanoseconds since epoch**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a177f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for tcol in ['start_time', 'end_time']:\n",
    "    if tcol in df.columns:\n",
    "        try:\n",
    "            mean_val = df[tcol].dropna().astype(float).mean()\n",
    "            if mean_val > 1e12:\n",
    "                df[tcol] = pd.to_datetime(df[tcol], unit='ns', origin='unix', errors='coerce')\n",
    "            else:\n",
    "                df[tcol] = pd.to_datetime(df[tcol], unit='s', origin='unix', errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not convert {tcol} -> datetime:\", e)\n",
    "\n",
    "[c for c in ['start_time','end_time'] if c in df.columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58609a27",
   "metadata": {},
   "source": [
    "\n",
    "## Mean CPU by Hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e22c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'start_time' in df.columns and 'cpu_rate' in df.columns:\n",
    "    ts = df[['start_time','cpu_rate']].dropna().set_index('start_time').sort_index()\n",
    "    hourly = ts['cpu_rate'].resample('1H').mean()\n",
    "    display(hourly.head())\n",
    "    plt.figure()\n",
    "    hourly.plot()\n",
    "    plt.title(\"Mean CPU rate by hour\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Mean CPU rate\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Missing 'start_time' and/or 'cpu_rate'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd19043d",
   "metadata": {},
   "source": [
    "\n",
    "## Lag Features (Predictive Signals)\n",
    "Downsample to 5-minute intervals, then create lagged features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e17d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'start_time' in df.columns and 'cpu_rate' in df.columns:\n",
    "    ts5 = df[['start_time','cpu_rate']].dropna().set_index('start_time').sort_index()['cpu_rate'].resample('5T').mean()\n",
    "    ts5 = ts5.interpolate(limit_direction='both')\n",
    "    lagged = pd.DataFrame({\n",
    "        'cpu_t': ts5,\n",
    "        'cpu_t_1': ts5.shift(1),\n",
    "        'cpu_t_2': ts5.shift(2),\n",
    "        'cpu_t_12': ts5.shift(12),  # ~1 hour lag at 5-min res\n",
    "    }).dropna()\n",
    "    display(lagged.head())\n",
    "    print(\"\\nLag correlations:\")\n",
    "    display(lagged.corr())\n",
    "else:\n",
    "    print(\"Cannot compute lag features without 'start_time' and 'cpu_rate'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fbc19a",
   "metadata": {},
   "source": [
    "\n",
    "## Grouping (Machine/Job/Task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841da601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "group_keys = [c for c in ['machine_id', 'job_id', 'task_index'] if c in df.columns]\n",
    "if 'cpu_rate' in df.columns and group_keys:\n",
    "    key = group_keys[0]\n",
    "    g = df[[key, 'cpu_rate']].dropna().groupby(key)['cpu_rate'].mean().sort_values(ascending=False).head(10)\n",
    "    print(f\"Top 10 {key} by mean CPU_rate:\")\n",
    "    display(g)\n",
    "else:\n",
    "    print(\"No grouping keys found or 'cpu_rate' missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0713e8",
   "metadata": {},
   "source": [
    "\n",
    "## Save Small Parquet Sample (optional)\n",
    "Saves a random subset for faster reloads. Will attempt to install `pyarrow` if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18520559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_parquet = \"/content/google_cluster_data/data_sample/sample_eda.parquet\"\n",
    "try:\n",
    "    df.sample(min(len(df), 200_000), random_state=17).to_parquet(out_parquet, index=False)\n",
    "    print(f\"Saved sample to {out_parquet}\")\n",
    "except Exception as e:\n",
    "    print(\"Parquet save failed; attempting to install pyarrow...\")\n",
    "    try:\n",
    "        import sys, subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyarrow\"])\n",
    "        df.sample(min(len(df), 200_000), random_state=17).to_parquet(out_parquet, index=False)\n",
    "        print(f\"Saved sample to {out_parquet}\")\n",
    "    except Exception as e2:\n",
    "        print(\"Parquet still unavailable. Skipping parquet export.\", e2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a01f2",
   "metadata": {},
   "source": [
    "\n",
    "### Next Steps\n",
    "- Concatenate additional `task_usage` parts for larger windows of time.\n",
    "- Join with `task_events` / `machine_events` to add scheduling and capacity context.\n",
    "- Engineer autoscaling features (rolling means/percentiles, sustained high-CPU windows).\n",
    "- Prototype scaling policies: threshold-based vs. predictive vs. RL.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
